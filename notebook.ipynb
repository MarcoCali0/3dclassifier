{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "DATA_DIR = \"ModelNet10\"\n",
    "SAVE_DIR = \"ModelNet10Voxel\"\n",
    "\n",
    "num_classes = 10\n",
    "grid_size = 32\n",
    "object_size = 24\n",
    "pitch_rescale = 1.0\n",
    "no_of_rotations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "class ModelNetDatasetVoxel(Dataset):\n",
    "    def __init__(self, data_dir, train=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.train = train\n",
    "        self.class_map = {}\n",
    "        self.files, self.labels = self.load_files_and_labels()\n",
    "\n",
    "    def load_files_and_labels(self):\n",
    "        folders = glob.glob(os.path.join(self.data_dir, \"*\"))\n",
    "        files = []\n",
    "        labels = []\n",
    "        for i, folder in enumerate(folders):\n",
    "            self.class_map[i] = os.path.basename(folder)\n",
    "            dataset_type = \"train\" if self.train else \"test\"\n",
    "            class_files = glob.glob(os.path.join(folder, f\"{dataset_type}/*.pt\"))\n",
    "            files.extend(class_files)\n",
    "            labels.extend([i] * len(class_files))\n",
    "        return files, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        voxel_grid = torch.load(file_path)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return voxel_grid, label\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = ModelNetDatasetVoxel(SAVE_DIR, train=True)\n",
    "test_dataset = ModelNetDatasetVoxel(SAVE_DIR, train=False)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x) if self.downsample else x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv3d: 1, BatchNorm3d: 1, ReLU: 1, Sequential: 1, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Flatten: 1, Dropout: 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mVoxResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1024 and 512x10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m model \u001b[38;5;241m=\u001b[39m VoxResNet(classes_num\u001b[38;5;241m=\u001b[39mnum_classes, voxelgrid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv3d: 1, BatchNorm3d: 1, ReLU: 1, Sequential: 1, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Sequential: 1, ResidualBlock: 2, Sequential: 3, Conv3d: 4, BatchNorm3d: 4, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, ResidualBlock: 2, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, Conv3d: 3, BatchNorm3d: 3, ReLU: 3, AvgPool3d: 2, Flatten: 1, Dropout: 1]"
     ]
    }
   ],
   "source": [
    "class VoxResNet(nn.Module):\n",
    "    def __init__(self, classes_num, in_channels=1, voxelgrid_size=(32, 32, 32)):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Residual blocks\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(16, 16),\n",
    "            ResidualBlock(16, 16),\n",
    "            ResidualBlock(16, 16),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(16, 32),\n",
    "            ResidualBlock(32, 32),\n",
    "            ResidualBlock(32, 32),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResidualBlock(64, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            ResidualBlock(128, 128),\n",
    "            nn.AvgPool3d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Flattening and final linear layer\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=int(voxelgrid_size[0] / 2**4 * voxelgrid_size[1] / 2**4 * 128),\n",
    "            out_features=classes_num,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# model = VoxResNet(ResidualBlock, [2, 2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "model = VoxResNet(classes_num=num_classes, voxelgrid_size=(32, 32, 32), in_channels=1)\n",
    "model.to(device)\n",
    "print(summary(model, input_size=(batch_size, 1, grid_size, grid_size, grid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distribution = np.zeros(num_classes)\n",
    "test_class_distribution = np.zeros(num_classes)\n",
    "for label in train_dataset.labels:\n",
    "    train_class_distribution[label] += 1\n",
    "\n",
    "for label in test_dataset.labels:\n",
    "    test_class_distribution[label] += 1\n",
    "\n",
    "\n",
    "weights = np.sum(train_class_distribution) / train_class_distribution\n",
    "weights /= np.sum(weights)\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################\n",
      "# EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 3, 3, 3], expected input[1, 16, 32, 32, 32] to have 1 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(out, batch_y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mVoxResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3, 3], expected input[1, 16, 32, 32, 32] to have 1 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "# Define your model, optimizer, and loss function\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Define the number of epochs and initialize logs\n",
    "num_epochs = 20\n",
    "train_loss_log = []\n",
    "val_loss_log = []\n",
    "best_val = np.inf\n",
    "start_epoch = 0\n",
    "\n",
    "# Check if there's a saved checkpoint\n",
    "checkpoint_path = \"checkpoint.pt\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    opt.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    train_loss_log = checkpoint[\"train_loss_log\"]\n",
    "    val_loss_log = checkpoint[\"val_loss_log\"]\n",
    "    best_val = checkpoint[\"best_val\"]\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "for epoch_num in range(start_epoch, num_epochs):\n",
    "    print(\"#################\")\n",
    "    print(f\"# EPOCH {epoch_num}\")\n",
    "\n",
    "    ### TRAIN\n",
    "    model.train()  # Training mode (e.g. enable dropout, batchnorm updates,...)\n",
    "    train_losses = []\n",
    "    iterator = tqdm(train_dataloader)\n",
    "    for batch_x, batch_y in iterator:\n",
    "        # Move data to device\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(batch_x)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out, batch_y)\n",
    "\n",
    "        # Backpropagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        opt.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        iterator.set_description(f\"Train loss: {round(loss.item(), 2)}\")\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    train_loss_log.append(avg_train_loss)\n",
    "\n",
    "    ### VALIDATION\n",
    "    model.eval()  # Evaluation mode (e.g. disable dropout, batchnorm,...)\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch_x, batch_y in tqdm(test_dataloader):\n",
    "            # Move data to device\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(batch_x)\n",
    "\n",
    "            val_losses.append(loss_fn(out, batch_y).item())\n",
    "\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_loss_log.append(avg_val_loss)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(\n",
    "        f\"Average validation loss: {round(avg_val_loss,2)}\\tValidation accuracy: {round(val_acc,2)}\"\n",
    "    )\n",
    "\n",
    "    # Save the model and optimizer state at each epoch\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch_num,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": opt.state_dict(),\n",
    "            \"train_loss_log\": train_loss_log,\n",
    "            \"val_loss_log\": val_loss_log,\n",
    "            \"best_val\": best_val,\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "\n",
    "    if avg_val_loss < best_val:\n",
    "        print(\"Update model!!!\")\n",
    "        torch.save(model.state_dict(), f\"best_model_{avg_val_loss}.pt\")\n",
    "        best_val = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.to(device)\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "iterator = tqdm(test_dataloader)\n",
    "for inputs, targets in iterator:\n",
    "    # Move data to device\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # Convert outputs to predicted labels\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Append to lists\n",
    "    all_predictions.extend(predicted.cpu().numpy())\n",
    "    all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(all_targets, all_predictions)\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
    "\n",
    "# Calculate accuracy per class\n",
    "class_names = [train_dataset.class_map.get(key) for key in range(num_classes)]\n",
    "class_report = classification_report(\n",
    "    all_targets, all_predictions, target_names=class_names\n",
    ")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print numerical values in each cell of the matrix\n",
    "fmt = \"d\"\n",
    "thresh = cm.max() / 2.0\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(\n",
    "        j,\n",
    "        i,\n",
    "        format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random samples\n",
    "num_samples = 5  # Number of random samples to predict\n",
    "random_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Get a random sample from the test set\n",
    "    sample, target = test_dataset[idx]\n",
    "    sample = sample.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Predict with the model\n",
    "    with torch.no_grad():\n",
    "        output = model(sample)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    # Convert predicted and target labels to numpy if needed\n",
    "    predicted_label = predicted.item()\n",
    "    true_label = target.item()  # Assuming target is a scalar label\n",
    "\n",
    "    # Plot the voxel grid\n",
    "    sample_np = (\n",
    "        sample.cpu().squeeze().numpy()\n",
    "    )  # Move sample to CPU and convert to numpy\n",
    "    ax = fig.add_subplot(1, num_samples, i + 1, projection=\"3d\")\n",
    "\n",
    "    # Voxel plot\n",
    "    sample_np = (sample_np + 1) / 2\n",
    "    ax.voxels(sample_np, edgecolor=\"k\")\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Pred: {train_dataset.class_map.get(predicted_label)}, True: {train_dataset.class_map.get(true_label)}\"\n",
    "    )\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_xlim(0, sample_np.shape[0])\n",
    "    ax.set_ylim(0, sample_np.shape[1])\n",
    "    ax.set_zlim(0, sample_np.shape[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class names from the dataset\n",
    "class_names = [train_dataset.class_map[i] for i in range(10)]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, tight_layout=True, figsize=(10, 8))\n",
    "\n",
    "# Plot training class distribution\n",
    "axs[0].bar(range(10), train_class_distribution, color=\"skyblue\")\n",
    "axs[0].set_xticks(range(num_classes))\n",
    "axs[0].set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "axs[0].set_title(\"Training Class Distribution\")\n",
    "axs[0].set_ylabel(\"Number of Samples\")\n",
    "\n",
    "# Plot test class distribution\n",
    "axs[1].bar(range(num_classes), test_class_distribution, color=\"salmon\")\n",
    "axs[1].set_xticks(range(num_classes))\n",
    "axs[1].set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "axs[1].set_title(\"Test Class Distribution\")\n",
    "axs[1].set_ylabel(\"Number of Samples\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
